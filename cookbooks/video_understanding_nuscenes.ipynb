{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07a5e83c",
   "metadata": {},
   "source": [
    "### Video Understanding with Qwen2.5-VL\n",
    "\n",
    "In this notebook, we delve into the capabilities of the **Qwen2.5-VL** model for video understanding tasks. Our objective is to showcase how this advanced model can be applied to various video analysis scenarios, ranging from basic OCR to complex event detection and summarization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223b154d",
   "metadata": {},
   "source": [
    "#### \\[Setup\\]\n",
    "\n",
    "We start by loading the pre-trained `Qwen2_5_VLForConditionalGeneration` model. This model has been fine-tuned on a diverse set of video understanding tasks, enabling it to generate detailed and accurate descriptions based on visual inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5623f17b",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "model_path = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d405fc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install eva-decord"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab78f266",
   "metadata": {},
   "source": [
    "Load video frames and timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f08173dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import math\n",
    "# import hashlib\n",
    "# import requests\n",
    "\n",
    "# from IPython.display import Markdown, display\n",
    "# import numpy as np\n",
    "# from PIL import Image\n",
    "# import decord\n",
    "# from decord import VideoReader, cpu\n",
    "\n",
    "\n",
    "# def download_video(url, dest_path):\n",
    "#     response = requests.get(url, stream=True)\n",
    "#     with open(dest_path, 'wb') as f:\n",
    "#         for chunk in response.iter_content(chunk_size=8096):\n",
    "#             f.write(chunk)\n",
    "#     print(f\"Video downloaded to {dest_path}\")\n",
    "\n",
    "\n",
    "# def get_video_frames(video_path, num_frames=128, cache_dir='.cache'):\n",
    "#     os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "#     video_hash = hashlib.md5(video_path.encode('utf-8')).hexdigest()\n",
    "#     if video_path.startswith('http://') or video_path.startswith('https://'):\n",
    "#         video_file_path = os.path.join(cache_dir, f'{video_hash}.mp4')\n",
    "#         if not os.path.exists(video_file_path):\n",
    "#             download_video(video_path, video_file_path)\n",
    "#     else:\n",
    "#         video_file_path = video_path\n",
    "\n",
    "#     frames_cache_file = os.path.join(cache_dir, f'{video_hash}_{num_frames}_frames.npy')\n",
    "#     timestamps_cache_file = os.path.join(cache_dir, f'{video_hash}_{num_frames}_timestamps.npy')\n",
    "\n",
    "#     if os.path.exists(frames_cache_file) and os.path.exists(timestamps_cache_file):\n",
    "#         frames = np.load(frames_cache_file)\n",
    "#         timestamps = np.load(timestamps_cache_file)\n",
    "#         return video_file_path, frames, timestamps\n",
    "\n",
    "#     vr = VideoReader(video_file_path, ctx=cpu(0))\n",
    "#     total_frames = len(vr)\n",
    "\n",
    "#     indices = np.linspace(0, total_frames - 1, num=num_frames, dtype=int)\n",
    "#     frames = vr.get_batch(indices).asnumpy()\n",
    "#     timestamps = np.array([vr.get_frame_timestamp(idx) for idx in indices])\n",
    "\n",
    "#     np.save(frames_cache_file, frames)\n",
    "#     np.save(timestamps_cache_file, timestamps)\n",
    "    \n",
    "#     return video_file_path, frames, timestamps\n",
    "\n",
    "\n",
    "# def create_image_grid(images, num_columns=8):\n",
    "#     pil_images = [Image.fromarray(image) for image in images]\n",
    "#     num_rows = math.ceil(len(images) / num_columns)\n",
    "\n",
    "#     img_width, img_height = pil_images[0].size\n",
    "#     grid_width = num_columns * img_width\n",
    "#     grid_height = num_rows * img_height\n",
    "#     grid_image = Image.new('RGB', (grid_width, grid_height))\n",
    "\n",
    "#     for idx, image in enumerate(pil_images):\n",
    "#         row_idx = idx // num_columns\n",
    "#         col_idx = idx % num_columns\n",
    "#         position = (col_idx * img_width, row_idx * img_height)\n",
    "#         grid_image.paste(image, position)\n",
    "\n",
    "#     return grid_image\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import hashlib\n",
    "import re\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def get_video_frames_nuscenes(scene_prefix, frames_dir, camera='CAM_FRONT', num_frames=128, cache_dir='.cache', fps=12):\n",
    "    \"\"\"\n",
    "    Load video frames from nuScenes sequential images.\n",
    "    \n",
    "    Args:\n",
    "        scene_prefix (str): Scene identifier prefix (e.g., 'n015-2018-11-21-19-38-26+0800')\n",
    "        frames_dir (str): Directory containing the image frames\n",
    "        camera (str): Camera name (default: 'CAM_FRONT')\n",
    "        num_frames (int): Number of frames to sample (default: 128)\n",
    "        cache_dir (str): Directory for caching processed frames (default: '.cache')\n",
    "        fps (float): Frame rate for timestamp calculation (default: 12)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (scene_path, frames, timestamps)\n",
    "            - scene_path: Path to the frames directory\n",
    "            - frames: numpy array of shape (num_frames, height, width, channels)\n",
    "            - timestamps: numpy array of timestamps for each frame\n",
    "    \"\"\"\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    \n",
    "    # Create a unique hash for this scene and camera combination\n",
    "    scene_hash = hashlib.md5(f\"{scene_prefix}_{camera}_{frames_dir}\".encode('utf-8')).hexdigest()\n",
    "    \n",
    "    frames_cache_file = os.path.join(cache_dir, f'{scene_hash}_{num_frames}_frames.npy')\n",
    "    timestamps_cache_file = os.path.join(cache_dir, f'{scene_hash}_{num_frames}_timestamps.npy')\n",
    "    \n",
    "    # Check if cached files exist\n",
    "    if os.path.exists(frames_cache_file) and os.path.exists(timestamps_cache_file):\n",
    "        frames = np.load(frames_cache_file)\n",
    "        timestamps = np.load(timestamps_cache_file)\n",
    "        return frames_dir, frames, timestamps\n",
    "    \n",
    "    # Find all image files matching the scene prefix and camera\n",
    "    pattern = os.path.join(frames_dir, f\"{scene_prefix}__{camera}__*.jpg\")\n",
    "    image_files = glob.glob(pattern)\n",
    "    \n",
    "    if not image_files:\n",
    "        raise ValueError(f\"No images found for scene '{scene_prefix}' and camera '{camera}' in directory '{frames_dir}'\")\n",
    "    \n",
    "    # Extract timestamps from filenames and sort\n",
    "    file_timestamp_pairs = []\n",
    "    for img_path in image_files:\n",
    "        filename = os.path.basename(img_path)\n",
    "        # Extract timestamp from filename pattern: prefix__camera__timestamp.jpg\n",
    "        match = re.match(rf\"{re.escape(scene_prefix)}__{re.escape(camera)}__(\\d+)\\.jpg\", filename)\n",
    "        if match:\n",
    "            timestamp = int(match.group(1))\n",
    "            file_timestamp_pairs.append((img_path, timestamp))\n",
    "    \n",
    "    if not file_timestamp_pairs:\n",
    "        raise ValueError(f\"No valid timestamps found in filenames for scene '{scene_prefix}'\")\n",
    "    \n",
    "    # Sort by timestamp\n",
    "    file_timestamp_pairs.sort(key=lambda x: x[1])\n",
    "    \n",
    "    total_frames = len(file_timestamp_pairs)\n",
    "    \n",
    "    # Sample frames at evenly spaced intervals\n",
    "    if num_frames >= total_frames:\n",
    "        # If we want more frames than available, use all frames\n",
    "        indices = list(range(total_frames))\n",
    "    else:\n",
    "        # Sample evenly across the sequence\n",
    "        indices = np.linspace(0, total_frames - 1, num=num_frames, dtype=int)\n",
    "    \n",
    "    # Load the selected frames\n",
    "    frames = []\n",
    "    timestamps = []\n",
    "    \n",
    "    for idx in indices:\n",
    "        img_path, timestamp_microseconds = file_timestamp_pairs[idx]\n",
    "        \n",
    "        # Load and convert image\n",
    "        img = Image.open(img_path)\n",
    "        img_array = np.array(img)\n",
    "        frames.append(img_array)\n",
    "        \n",
    "        # Convert timestamp from microseconds to seconds (as tuples like the original function)\n",
    "        timestamp_seconds = timestamp_microseconds / 1_000_000.0\n",
    "        timestamps.append((timestamp_seconds, timestamp_seconds))  # (start, end) format like original\n",
    "    \n",
    "    frames = np.array(frames)\n",
    "    timestamps = np.array(timestamps)\n",
    "    \n",
    "    # Cache the results\n",
    "    np.save(frames_cache_file, frames)\n",
    "    np.save(timestamps_cache_file, timestamps)\n",
    "    \n",
    "    return frames_dir, frames, timestamps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ffbbc2",
   "metadata": {},
   "source": [
    "Inference function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ef59a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(video_path, prompt, max_new_tokens=2048, total_pixels=20480 * 28 * 28, min_pixels=16 * 28 * 28):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"text\", \"text\": prompt},\n",
    "                {\"video\": video_path, \"total_pixels\": total_pixels, \"min_pixels\": min_pixels},\n",
    "            ]\n",
    "        },\n",
    "    ]\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    image_inputs, video_inputs, video_kwargs = process_vision_info([messages], return_video_kwargs=True)\n",
    "    fps_inputs = video_kwargs['fps']\n",
    "    print(\"video input:\", video_inputs[0].shape)\n",
    "    num_frames, _, resized_height, resized_width = video_inputs[0].shape\n",
    "    print(\"num of video tokens:\", int(num_frames / 2 * resized_height / 28 * resized_width / 28))\n",
    "    inputs = processor(text=[text], images=image_inputs, videos=video_inputs, fps=fps_inputs, padding=True, return_tensors=\"pt\")\n",
    "    inputs = inputs.to('cuda')\n",
    "\n",
    "    output_ids = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "    generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]\n",
    "    output_text = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    return output_text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5057b6a8",
   "metadata": {},
   "source": [
    "Inference function with API using OpenAI SDK.\n",
    "\n",
    "**Important Notice:**\n",
    "- Please be aware that the current API supports video processing up to a maximum length of 10 minutes.\n",
    "- Currently, the model inference interface does not support configuring the resolution of video frames. Therefore, it is recommended to resize videos with higher resolutions and longer durations to a smaller resolution to ensure that the input sequence is not excessively long. We advise keeping the number of video tokens under 24k to achieve better video grounding results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4c0a3ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "\n",
    "def inference_with_api(\n",
    "    video_path,\n",
    "    prompt,\n",
    "    sys_prompt = \"You are a helpful assistant.\",\n",
    "    model_id = \"qwen/qwen-vl-max\",\n",
    "    # model_id=\"qwen/qwen2.5-vl-72b-instruct:free\",\n",
    "):\n",
    "    client = OpenAI(\n",
    "        api_key=os.getenv('OPENROUTER_API_KEY'),\n",
    "        base_url=\"https://openrouter.ai/api/v1\",\n",
    "    )    \n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\":\"text\",\"text\": sys_prompt}]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"video_url\", \"video_url\": {\"url\": video_path}},\n",
    "                {\"type\": \"text\", \"text\": prompt},\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    completion = client.chat.completions.create(\n",
    "        model = model_id,\n",
    "        messages = messages,\n",
    "    )\n",
    "    print(\"video url:\", video_path)\n",
    "    print(completion)\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58729648",
   "metadata": {},
   "source": [
    "#### 1. Reading Text in Videos\n",
    "\n",
    "In this section, we demonstrate how the model can be used to recognize and summarize text within a video. Specifically, we'll use a video containing various products and ask the model to summarize their characteristics in a structured format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa848e7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/lilyzhang/Desktop/Qwen2.5-VL/v1.0-mini/videos/n008-2018-08-01-15-16-36-0400.mp4'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def convert_nuscenes_to_mp4(scene_prefix, frames_dir, output_path, camera='CAM_FRONT', fps=12):\n",
    "    \"\"\"\n",
    "    Convert nuScenes frame sequence to MP4 video\n",
    "    \"\"\"\n",
    "    # Get all frames for the scene\n",
    "    pattern = os.path.join(frames_dir, f\"{scene_prefix}__{camera}__*.jpg\")\n",
    "    image_files = sorted(glob.glob(pattern))\n",
    "    \n",
    "    if not image_files:\n",
    "        raise ValueError(f\"No images found for scene '{scene_prefix}'\")\n",
    "    \n",
    "    # Read first image to get dimensions\n",
    "    first_img = cv2.imread(image_files[0])\n",
    "    height, width, layers = first_img.shape\n",
    "    \n",
    "    # Define codec and create VideoWriter\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    video_writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    # Write frames to video\n",
    "    for img_path in image_files:\n",
    "        frame = cv2.imread(img_path)\n",
    "        video_writer.write(frame)\n",
    "    \n",
    "    video_writer.release()\n",
    "    return output_path\n",
    "\n",
    "# Usage\n",
    "scene_prefix = \"n008-2018-08-01-15-16-36-0400\"\n",
    "frames_dir = \"/Users/lilyzhang/Desktop/Qwen2.5-VL/v1.0-mini/sweeps/CAM_FRONT\"\n",
    "output_video = f\"/Users/lilyzhang/Desktop/Qwen2.5-VL/v1.0-mini/videos/{scene_prefix}.mp4\"\n",
    "\n",
    "# Convert to MP4\n",
    "convert_nuscenes_to_mp4(scene_prefix, frames_dir, output_video, fps=12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0379e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "08ec89f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video url: https://raw.githubusercontent.com/lilyzhng/Qwen2.5-VL/ba8a0f136c56310cdd2ed2fad9bef7c37a48fcba/v1.0-mini/videos/n008-2018-08-01-15-16-36-0400.mp4\n",
      "ChatCompletion(id='gen-1751263895-gKm2pbR6ANVo1LCNAdLH', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"I'm sorry, but I'm not able to provide information about a specific video without more context. Could you please provide me with the title of the video, the name of the creator or channel, or any other relevant details? This will help me better understand which video you are referring to and provide you with more accurate information.\", refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, reasoning=None), native_finish_reason='stop')], created=1751263895, model='qwen/qwen-vl-max', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=66, prompt_tokens=28, total_tokens=94, completion_tokens_details=None, prompt_tokens_details=None), provider='Alibaba')\n"
     ]
    }
   ],
   "source": [
    "def upload_to_github_and_get_url():\n",
    "    \"\"\"\n",
    "    Helper function to generate GitHub raw URL\n",
    "    You'll need to manually upload the file to GitHub first\n",
    "    \"\"\"\n",
    "    # Format: https://raw.githubusercontent.com/user/repo/branch/path\n",
    "    # Example implementation - you'd need to replace with your actual details\n",
    "    \n",
    "    username = \"lilyzhng\"\n",
    "    repo_name = \"Qwen2.5-VL\" \n",
    "    branch = \"benchmark-hard-examples\"\n",
    "    file_path = \"v1.0-mini/videos/n008-2018-08-01-15-16-36-0400.mp4\"  # path within repo\n",
    "    \n",
    "    raw_url = f\"https://raw.githubusercontent.com/{username}/{repo_name}/{branch}/{file_path}\"\n",
    "    return raw_url\n",
    "\n",
    "# Usage\n",
    "raw_url = upload_to_github_and_get_url()\n",
    "\n",
    "# video_url = \"https://ofasys-multimodal-wlcb-3.oss-cn-wulanchabu.aliyuncs.com/sibo.ssb/datasets/cookbook/ead2e3f0e7f836c9ec51236befdaf2d843ac13a6.mp4\"\n",
    "\n",
    "# video_url = \"https://github.com/lilyzhng/Qwen2.5-VL/blob/ba8a0f136c56310cdd2ed2fad9bef7c37a48fcba/v1.0-mini/videos/n008-2018-08-01-15-16-36-0400.mp4\"\n",
    "\n",
    "video_url = \"https://raw.githubusercontent.com/lilyzhng/Qwen2.5-VL/ba8a0f136c56310cdd2ed2fad9bef7c37a48fcba/v1.0-mini/videos/n008-2018-08-01-15-16-36-0400.mp4\"\n",
    "# github_url = \"https://raw.githubusercontent.com/yourusername/nuscenes-videos/main/n008-2018-08-01-15-16-36-0400.mp4\"\n",
    "# prompt = \"Localize a series of activity events in the video, output the start and end timestamp for each event, and describe each event with sentences. Provide the result in json format with ‘mm:ss.ff’ format for time depiction.\"\n",
    "\n",
    "# prompt = \"Localize different video driving behaviors in the 16 seconds video, output the start and end timestamp for each event, and describe each event with sentences. Provide the result in json format with ‘mm:ss.ff’ format for time depiction.\"\n",
    "prompt = \"Could you go into detail about the content this video?\"\n",
    "response = inference_with_api(video_path = video_url, prompt=prompt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bd6fa912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Trying Format 1 ---\n",
      "Response: To provide a detailed analysis of a nuScenes driving video, I would typically need to view the specific video in question. However, since the video itself isn't provided, I can offer a general analysi...\n",
      "✅ This format seems to work!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "def debug_video_api_call(video_url, prompt):\n",
    "    \"\"\"Enhanced debugging for video API calls\"\"\"\n",
    "    \n",
    "    client = OpenAI(\n",
    "        api_key=os.getenv('OPENROUTER_API_KEY'),\n",
    "        base_url=\"https://openrouter.ai/api/v1\",\n",
    "    )\n",
    "    \n",
    "    # Try different message formats\n",
    "    formats_to_try = [\n",
    "        # Format 1: video_url type\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"video_url\", \"video_url\": {\"url\": video_url}},\n",
    "                {\"type\": \"text\", \"text\": prompt},\n",
    "            ]\n",
    "        },\n",
    "        # Format 2: video type (might work better)\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": [\n",
    "                {\"type\": \"video\", \"video\": video_url},\n",
    "                {\"type\": \"text\", \"text\": prompt},\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for i, user_message in enumerate(formats_to_try):\n",
    "        print(f\"\\n--- Trying Format {i+1} ---\")\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            user_message\n",
    "        ]\n",
    "        \n",
    "        try:\n",
    "            completion = client.chat.completions.create(\n",
    "                model=\"qwen/qwen-vl-max\",\n",
    "                messages=messages,\n",
    "            )\n",
    "            \n",
    "            response = completion.choices[0].message.content\n",
    "            print(f\"Response: {response[:200]}...\")\n",
    "            \n",
    "            # Check if response seems to actually describe video content\n",
    "            if any(word in response.lower() for word in ['video', 'scene', 'frame', 'motion', 'visual', 'driving', 'car', 'road']):\n",
    "                print(\"✅ This format seems to work!\")\n",
    "                return response\n",
    "            else:\n",
    "                print(\"❌ Generic response - format might not work\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error with format {i+1}: {e}\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Test with your working URL\n",
    "video_url = \"https://raw.githubusercontent.com/lilyzhng/Qwen2.5-VL/ba8a0f136c56310cdd2ed2fad9bef7c37a48fcba/v1.0-mini/videos/n008-2018-08-01-15-16-36-0400.mp4\"\n",
    "prompt = \"Analyze this nuScenes driving video. Describe the road scene, vehicles, pedestrians, traffic signs, and any notable events you observe. Be specific about what you see.\"\n",
    "\n",
    "result = debug_video_api_call(video_url, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "90e6076a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing Video 1: http://ofasys-multimodal-wlcb-3.oss-cn-wulanchabu.... ---\n",
      "✅ API successfully processed video!\n",
      "Response: I'm sorry, but as an AI language model, I don't have the ability to see or access videos. Can you please provide me with more information or a description of the video you are referring to? I'll do my...\n",
      "\n",
      "--- Testing Video 2: https://ofasys-multimodal-wlcb-3.oss-cn-wulanchabu... ---\n",
      "✅ API successfully processed video!\n",
      "Response: I'm sorry, but as an AI language model, I don't have the ability to see or access videos. Can you please provide me with more information or a description of the video you are referring to? I'll do my...\n",
      "\n",
      "--- Testing Video 3: https://raw.githubusercontent.com/lilyzhng/Qwen2.5... ---\n",
      "✅ API successfully processed video!\n",
      "Response: I'm sorry, but as an AI language model, I don't have the ability to see or access videos. Can you please provide me with more information or a description of the video you are referring to? I'll do my...\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI(\n",
    "    api_key=os.getenv('OPENROUTER_API_KEY'),\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    ")\n",
    "\n",
    "# Try with a known working video URL first\n",
    "test_video_urls = [\n",
    "    \"http://ofasys-multimodal-wlcb-3.oss-cn-wulanchabu.aliyuncs.com/sibo.ssb/cookbook/video_ocr.mp4\",  # Small test video\n",
    "     \"https://ofasys-multimodal-wlcb-3.oss-cn-wulanchabu.aliyuncs.com/sibo.ssb/datasets/cookbook/ead2e3f0e7f836c9ec51236befdaf2d843ac13a6.mp4\", #test\n",
    "    video_url  # Your GitHub URL\n",
    "]\n",
    "\n",
    "for i, test_url in enumerate(test_video_urls):\n",
    "    print(f\"\\n--- Testing Video {i+1}: {test_url[:50]}... ---\")\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"video_url\", \"video_url\": {\"url\": test_url}},\n",
    "                {\"type\": \"text\", \"text\": \"Describe what you see in this video.\"},\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"qwen/qwen-vl-max\",\n",
    "            messages=messages,\n",
    "        )\n",
    "        response = completion.choices[0].message.content\n",
    "        \n",
    "        if \"video itself isn't provided\" in response or \"I would typically need\" in response:\n",
    "            print(\"❌ API can't access this video\")\n",
    "        else:\n",
    "            print(\"✅ API successfully processed video!\")\n",
    "            print(f\"Response: {response[:200]}...\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1e233a",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_url = \"https://duguang-labelling.oss-cn-shanghai.aliyuncs.com/qiansun/video_ocr/videos/50221078283.mp4\"\n",
    "prompt = \"请用表格总结一下视频中的商品特点\"\n",
    "\n",
    "## Use a local HuggingFace model to inference.\n",
    "video_path, frames, timestamps = get_video_frames(video_url, num_frames=64)\n",
    "# image_grid = create_image_grid(frames, num_columns=8)\n",
    "# display(image_grid.resize((640, 640)))\n",
    "\n",
    "response = inference(video_path, prompt)\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "970b12d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video url: http://ofasys-multimodal-wlcb-3.oss-cn-wulanchabu.aliyuncs.com/sibo.ssb/cookbook/video_ocr.mp4\n",
      "ChatCompletion(id='gen-1751264312-3ZDVXOXS3vNCcWjGT5By', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Since you haven\\'t provided a specific video for me to watch, I\\'ll create a hypothetical example based on common research paper titles in the field of artificial intelligence. Here\\'s a table listing the paper titles along with their Chinese translations:\\n\\n| Original Paper Title                                      | Chinese Translation                                |\\n|-----------------------------------------------------------|----------------------------------------------------|\\n| \"Attention is All You Need\"                               | \"注意力是你所需要的全部\"                           |\\n| \"Deep Residual Learning for Image Recognition\"            | \"用于图像识别的深度残差学习\"                       |\\n| \"Generative Adversarial Networks\"                         | \"生成对抗网络\"                                     |\\n| \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" | \"BERT：用于语言理解的深度双向变压器预训练\"         |\\n| \"Adam: A Method for Stochastic Optimization\"              | \"Adam：一种随机优化方法\"                           |\\n| \"ImageNet Classification with Deep Convolutional Neural Networks\" | \"使用深度卷积神经网络进行ImageNet分类\"             |\\n| \"Long Short-Term Memory\"                                  | \"长短期记忆网络\"                                   |\\n| \"Playing Atari with Deep Reinforcement Learning\"          | \"使用深度强化学习玩Atari游戏\"                      |\\n| \"Auto-Encoding Variational Bayes\"                         | \"自动编码变分贝叶斯\"                               |\\n| \"Very Deep Convolutional Networks for Large-Scale Image Recognition\" | \"用于大规模图像识别的非常深的卷积网络\"             |\\n\\n### Explanation:\\n- **Original Paper Title**: This column lists the titles of the research papers in English.\\n- **Chinese Translation**: This column provides the Chinese translation of the corresponding paper titles.\\n\\nIf you have a specific video in mind, please share it, and I can create a table based on the actual content of the video.', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, reasoning=None), native_finish_reason='stop')], created=1751264312, model='qwen/qwen-vl-max', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=389, prompt_tokens=41, total_tokens=430, completion_tokens_details=None, prompt_tokens_details=None), provider='Alibaba')\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Since you haven't provided a specific video for me to watch, I'll create a hypothetical example based on common research paper titles in the field of artificial intelligence. Here's a table listing the paper titles along with their Chinese translations:\n",
       "\n",
       "| Original Paper Title                                      | Chinese Translation                                |\n",
       "|-----------------------------------------------------------|----------------------------------------------------|\n",
       "| \"Attention is All You Need\"                               | \"注意力是你所需要的全部\"                           |\n",
       "| \"Deep Residual Learning for Image Recognition\"            | \"用于图像识别的深度残差学习\"                       |\n",
       "| \"Generative Adversarial Networks\"                         | \"生成对抗网络\"                                     |\n",
       "| \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" | \"BERT：用于语言理解的深度双向变压器预训练\"         |\n",
       "| \"Adam: A Method for Stochastic Optimization\"              | \"Adam：一种随机优化方法\"                           |\n",
       "| \"ImageNet Classification with Deep Convolutional Neural Networks\" | \"使用深度卷积神经网络进行ImageNet分类\"             |\n",
       "| \"Long Short-Term Memory\"                                  | \"长短期记忆网络\"                                   |\n",
       "| \"Playing Atari with Deep Reinforcement Learning\"          | \"使用深度强化学习玩Atari游戏\"                      |\n",
       "| \"Auto-Encoding Variational Bayes\"                         | \"自动编码变分贝叶斯\"                               |\n",
       "| \"Very Deep Convolutional Networks for Large-Scale Image Recognition\" | \"用于大规模图像识别的非常深的卷积网络\"             |\n",
       "\n",
       "### Explanation:\n",
       "- **Original Paper Title**: This column lists the titles of the research papers in English.\n",
       "- **Chinese Translation**: This column provides the Chinese translation of the corresponding paper titles.\n",
       "\n",
       "If you have a specific video in mind, please share it, and I can create a table based on the actual content of the video."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Use API for inference. Apply API key here: https://bailian.console.alibabacloud.com/?apiKey=1\n",
    "# os.environ['DASHSCOPE_API_KEY'] = 'your_api_key_here' \n",
    "\n",
    "video_url = \"http://ofasys-multimodal-wlcb-3.oss-cn-wulanchabu.aliyuncs.com/sibo.ssb/cookbook/video_ocr.mp4\"\n",
    "prompt = \"Watch the video and list the paper titles in a table, add one extra column for translating the paper titles to Chinese.\"\n",
    "os.environ['OPENROUTER_API_KEY'] ='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'\n",
    "response = inference_with_api(video_url, prompt)\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f368545",
   "metadata": {},
   "source": [
    "#### 2. Long Video Understanding\n",
    "\n",
    "Next, we explore the model's capability to comprehend extremely long videos, such as those lasting up to one hour. This demonstrates how the model can effectively process and analyze extended video content, extracting meaningful insights over longer durations.\n",
    "\n",
    "To reduce the number of visual tokens generated from a long video, you can specify the `resized_height` and `resized_width` parameters. These settings allow the video frames to be resized to a smaller dimension, effectively decreasing the computational load while maintaining the essential visual information needed for analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030041db",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_url = \"https://ofasys-multimodal-wlcb-3.oss-cn-wulanchabu.aliyuncs.com/sibo.ssb/evaluations/data/LVBench/videos/GcRKREorGSc.mp4\"\n",
    "prompt = \"Could you go into detail about the content of this long video?\"\n",
    "\n",
    "video_path, frames, timestamps = get_video_frames(video_url, num_frames=64)\n",
    "# image_grid = create_image_grid(frames, num_columns=8)\n",
    "# display(image_grid.resize((640, 640)))\n",
    "\n",
    "response = inference(video_path, prompt)\n",
    "display(Markdown(response))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610b0b05",
   "metadata": {},
   "source": [
    "#### 3. Video Grounding\n",
    "\n",
    "This part focuses on answering specific questions about a video segment. We specify a textual query and ask the model what is the period that the described content occur in the video, showcasing the model's ability to understand timestamps and search the detailed queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebe81ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_url = \"https://ofasys-multimodal-wlcb-3.oss-cn-wulanchabu.aliyuncs.com/sibo.ssb/datasets/cookbook/ead2e3f0e7f836c9ec51236befdaf2d843ac13a6.mp4\"\n",
    "prompt = \"Give the query: 'seasoning the steak', when does the described content occur in the video?\"\n",
    "\n",
    "video_path, frames, timestamps = get_video_frames(video_url, num_frames=64)\n",
    "image_grid = create_image_grid(frames, num_columns=8)\n",
    "display(image_grid.resize((640, 640)))\n",
    "\n",
    "# inference\n",
    "response = inference(video_path, prompt)\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9228a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use API for inference. Apply API key here: https://bailian.console.alibabacloud.com/?apiKey=1\n",
    "# os.environ['DASHSCOPE_API_KEY'] = 'your_api_key_here' \n",
    "\n",
    "video_url = \"http://ofasys-multimodal-wlcb-3.oss-cn-wulanchabu.aliyuncs.com/sibo.ssb/cookbook/video_structured_caption_480p.mov\"\n",
    "prompt = \"Give the query: 'The seasoned meat is placed on a grill', when does the described content occur in the video? Use ‘mm:ss.ff’ as time format.\"\n",
    "\n",
    "response = inference_with_api(video_url, prompt)\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec0cab5",
   "metadata": {},
   "source": [
    "#### 4. Structured Video Captioning\n",
    "\n",
    "Finally, we present a scenario where the model identifies significant events within the video, providing start and end timestamps for each event along with descriptive sentences. The output is formatted in JSON for easy parsing and further processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd629f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_url = \"https://ofasys-multimodal-wlcb-3.oss-cn-wulanchabu.aliyuncs.com/sibo.ssb/datasets/cookbook/ead2e3f0e7f836c9ec51236befdaf2d843ac13a6.mp4\"\n",
    "prompt = \"Localize a series of activity events in the video, output the start and end timestamp for each event, and describe each event with sentences. Provide the result in json format with 'mm:ss.ff' format for time depiction.\"\n",
    "\n",
    "video_path, frames, timestamps = get_video_frames(video_url, num_frames=64)\n",
    "# image_grid = create_image_grid(frames, num_columns=8)\n",
    "# display(image_grid.resize((640, 640)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e24e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference\n",
    "response = inference(video_path, prompt)\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f485089",
   "metadata": {},
   "source": [
    "- By post-processing the json results, we can intuitively present video clips and descriptions in an interleaved manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9beb0368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def parse_json(response):\n",
    "    html = markdown.markdown(response, extensions=['fenced_code'])\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    json_text = soup.find('code').text\n",
    "\n",
    "    data = json.loads(json_text)\n",
    "    return data\n",
    "\n",
    "\n",
    "def time_to_seconds(time_str):\n",
    "    time_obj = datetime.strptime(time_str, '%M:%S.%f')\n",
    "    total_seconds = time_obj.minute * 60 + time_obj.second + time_obj.microsecond / 1_000_000\n",
    "    return total_seconds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14354bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = parse_json(response)\n",
    "\n",
    "for item in data:\n",
    "    start_time = item[\"start_time\"]\n",
    "    end_time = item[\"end_time\"]\n",
    "    description = item[\"description\"]\n",
    "\n",
    "    display(Markdown(f\"**{start_time} - {end_time}:**\\t\\t\" + description))\n",
    "\n",
    "    start_time = time_to_seconds(start_time)\n",
    "    end_time = time_to_seconds(end_time)\n",
    "    current_frames = []\n",
    "    for frame, timestamp in zip(frames, timestamps):\n",
    "        if timestamp[0] > start_time and timestamp[1] < end_time:\n",
    "            current_frames.append(frame)\n",
    "    \n",
    "    current_frames = np.array(current_frames)\n",
    "    current_image_grid = create_image_grid(current_frames, num_columns=8)\n",
    "\n",
    "    display(current_image_grid.resize((480, (int(len(current_frames) / 8) + 1) * 60)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cd4e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use API for inference. Apply API key here: https://bailian.console.alibabacloud.com/?apiKey=1\n",
    "# os.environ['DASHSCOPE_API_KEY'] = 'your_api_key_here' \n",
    "\n",
    "video_url = \"http://ofasys-multimodal-wlcb-3.oss-cn-wulanchabu.aliyuncs.com/sibo.ssb/cookbook/video_structured_caption_480p.mov\"\n",
    "prompt = \"Localize a series of activity events in the video, output the start and end timestamp for each event, and describe each event with sentences. Provide the result in json format with ‘mm:ss.ff’ format for time depiction.\"\n",
    "\n",
    "response = inference_with_api(video_url, prompt)\n",
    "display(Markdown(response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwen_env_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
