{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07a5e83c",
   "metadata": {},
   "source": [
    "### Video Understanding with Qwen2.5-VL\n",
    "\n",
    "In this notebook, we delve into the capabilities of the **Qwen2.5-VL** model for video understanding tasks. Our objective is to showcase how this advanced model can be applied to various video analysis scenarios, ranging from basic OCR to complex event detection and summarization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223b154d",
   "metadata": {},
   "source": [
    "#### \\[Setup\\]\n",
    "\n",
    "We start by loading the pre-trained `Qwen2_5_VLForConditionalGeneration` model. This model has been fine-tuned on a diverse set of video understanding tasks, enabling it to generate detailed and accurate descriptions based on visual inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5623f17b",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5df1ec6028347cb8b16210f4db64863",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "model_path = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2353dc57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is not on cuda, moving to cuda\n"
     ]
    }
   ],
   "source": [
    "# check model is already on cuda\n",
    "current_device = next(model.parameters()).device\n",
    "target_device = torch.device('cuda')\n",
    "\n",
    "if current_device != target_device:\n",
    "    print(\"Model is not on cuda, moving to cuda\")\n",
    "    model = model.to(target_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab78f266",
   "metadata": {},
   "source": [
    "Load video frames and timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d111f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install decord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fba3c169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from qwen_vl_utils import process_vision_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f08173dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import hashlib\n",
    "import requests\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import decord\n",
    "from decord import VideoReader, cpu\n",
    "\n",
    "def convert_gdrive_url_to_download_url(gdrive_url):\n",
    "    # Extract the file ID from the Gdrive URL\n",
    "    file_id = gdrive_url.split('/')[-2]\n",
    "    # Create the download URL\n",
    "    download_url = f\"https://drive.google.com/uc?export=download&id={file_id}\"\n",
    "    return download_url\n",
    "\n",
    "def download_video(url, dest_path):\n",
    "    response = requests.get(url, stream=True)\n",
    "    with open(dest_path, 'wb') as f:\n",
    "        for chunk in response.iter_content(chunk_size=8096):\n",
    "            f.write(chunk)\n",
    "    print(f\"Video downloaded to {dest_path}\")\n",
    "\n",
    "\n",
    "def get_video_frames(video_path, num_frames=128, cache_dir='.cache'):\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "    video_hash = hashlib.md5(video_path.encode('utf-8')).hexdigest()\n",
    "    if video_path.startswith('http://') or video_path.startswith('https://'):\n",
    "        video_file_path = os.path.join(cache_dir, f'{video_hash}.mp4')\n",
    "        if not os.path.exists(video_file_path):\n",
    "            download_video(video_path, video_file_path)\n",
    "    else:\n",
    "        video_file_path = video_path\n",
    "\n",
    "    frames_cache_file = os.path.join(cache_dir, f'{video_hash}_{num_frames}_frames.npy')\n",
    "    timestamps_cache_file = os.path.join(cache_dir, f'{video_hash}_{num_frames}_timestamps.npy')\n",
    "\n",
    "    if os.path.exists(frames_cache_file) and os.path.exists(timestamps_cache_file):\n",
    "        frames = np.load(frames_cache_file)\n",
    "        timestamps = np.load(timestamps_cache_file)\n",
    "        return video_file_path, frames, timestamps\n",
    "\n",
    "    vr = VideoReader(video_file_path, ctx=cpu(0))\n",
    "    total_frames = len(vr)\n",
    "\n",
    "    indices = np.linspace(0, total_frames - 1, num=num_frames, dtype=int)\n",
    "    frames = vr.get_batch(indices).asnumpy()\n",
    "    timestamps = np.array([vr.get_frame_timestamp(idx) for idx in indices])\n",
    "\n",
    "    np.save(frames_cache_file, frames)\n",
    "    np.save(timestamps_cache_file, timestamps)\n",
    "    \n",
    "    return video_file_path, frames, timestamps\n",
    "\n",
    "\n",
    "def create_image_grid(images, num_columns=8):\n",
    "    pil_images = [Image.fromarray(image) for image in images]\n",
    "    num_rows = math.ceil(len(images) / num_columns)\n",
    "\n",
    "    img_width, img_height = pil_images[0].size\n",
    "    grid_width = num_columns * img_width\n",
    "    grid_height = num_rows * img_height\n",
    "    grid_image = Image.new('RGB', (grid_width, grid_height))\n",
    "\n",
    "    for idx, image in enumerate(pil_images):\n",
    "        row_idx = idx // num_columns\n",
    "        col_idx = idx % num_columns\n",
    "        position = (col_idx * img_width, row_idx * img_height)\n",
    "        grid_image.paste(image, position)\n",
    "\n",
    "    return grid_image\n",
    "\n",
    "\n",
    "\n",
    "def get_video_embeddings(video_path, model, processor, num_frames=128, cache_dir='.cache',\n",
    "                        total_pixels=20480 * 28 * 28, min_pixels=16 * 28 * 28, \n",
    "                        original_fps=None, max_frames=30, include_mapping=True):\n",
    "    \"\"\"\n",
    "    Extended version that caches both frames and embeddings with FPS and timestamp mapping\n",
    "    \n",
    "    Args:\n",
    "        video_path: Path to video file\n",
    "        model: Qwen model\n",
    "        processor: Qwen processor\n",
    "        num_frames: Number of frames to extract (will be limited by max_frames if include_mapping=True)\n",
    "        cache_dir: Directory for caching\n",
    "        total_pixels: Total pixels for processing\n",
    "        min_pixels: Minimum pixels for processing\n",
    "        original_fps: Original video FPS (auto-detected if None)\n",
    "        max_frames: Maximum frames for model input (used when include_mapping=True)\n",
    "        include_mapping: Whether to include timestamp mapping functionality\n",
    "    \n",
    "    Returns:\n",
    "        frames: Video frames\n",
    "        timestamps: Frame timestamps\n",
    "        embeddings_data: Embeddings with additional metadata\n",
    "        mapping_info: Timestamp mapping information (if include_mapping=True)\n",
    "    \"\"\"\n",
    "    \n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    \n",
    "    # Auto-detect original FPS if not provided\n",
    "    if original_fps is None:\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        original_fps = cap.get(cv2.CAP_PROP_FPS) or 12.0  # Default fallback\n",
    "        cap.release()\n",
    "        print(f\"Auto-detected FPS: {original_fps}\")\n",
    "    \n",
    "    # Adjust num_frames based on mapping requirements\n",
    "    effective_num_frames = min(num_frames, max_frames) if include_mapping else num_frames\n",
    "    \n",
    "    # Create hash including all parameters including FPS\n",
    "    cache_params = f\"{video_path}_{effective_num_frames}_{total_pixels}_{min_pixels}_{original_fps}_{include_mapping}\"\n",
    "    video_hash = hashlib.md5(cache_params.encode('utf-8')).hexdigest()\n",
    "    \n",
    "    # Cache files\n",
    "    frames_cache_file = os.path.join(cache_dir, f'{video_hash}_frames.npy')\n",
    "    timestamps_cache_file = os.path.join(cache_dir, f'{video_hash}_timestamps.npy')\n",
    "    embeddings_cache_file = os.path.join(cache_dir, f'{video_hash}_embeddings.pt')\n",
    "    mapping_cache_file = os.path.join(cache_dir, f'{video_hash}_mapping.json')\n",
    "    \n",
    "    # Check if all cached files exist\n",
    "    cache_exists = (os.path.exists(frames_cache_file) and \n",
    "                   os.path.exists(timestamps_cache_file) and \n",
    "                   os.path.exists(embeddings_cache_file))\n",
    "    \n",
    "    if include_mapping:\n",
    "        cache_exists = cache_exists and os.path.exists(mapping_cache_file)\n",
    "    \n",
    "    if cache_exists:\n",
    "        print(f\"Loading cached video embeddings from {embeddings_cache_file}\")\n",
    "        frames = np.load(frames_cache_file)\n",
    "        timestamps = np.load(timestamps_cache_file)\n",
    "        embeddings_data = torch.load(embeddings_cache_file)\n",
    "        \n",
    "        if include_mapping:\n",
    "            import json\n",
    "            with open(mapping_cache_file, 'r') as f:\n",
    "                mapping_info = json.load(f)\n",
    "            return frames, timestamps, embeddings_data, mapping_info\n",
    "        else:\n",
    "            return frames, timestamps, embeddings_data\n",
    "    \n",
    "    # Get video properties for mapping\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    detected_fps = cap.get(cv2.CAP_PROP_FPS) or original_fps\n",
    "    duration = total_frames / detected_fps\n",
    "    cap.release()\n",
    "    \n",
    "    print(f\"Video info: {total_frames} frames, {detected_fps:.2f} FPS, {duration:.2f}s\")\n",
    "    \n",
    "    # Create mapping info if requested\n",
    "    mapping_info = None\n",
    "    if include_mapping:\n",
    "        # Calculate frame sampling for timestamp mapping\n",
    "        if total_frames <= effective_num_frames:\n",
    "            frame_indices = list(range(total_frames))\n",
    "        else:\n",
    "            frame_indices = np.linspace(0, total_frames - 1, effective_num_frames, dtype=int)\n",
    "        \n",
    "        mapping_info = {\n",
    "            'original_fps': detected_fps,\n",
    "            'original_total_frames': total_frames,\n",
    "            'original_duration': duration,\n",
    "            'sampled_frames': len(frame_indices),\n",
    "            'effective_num_frames': effective_num_frames,\n",
    "            'time_scale_factor': duration / (len(frame_indices) / original_fps) if len(frame_indices) > 1 else 1.0,\n",
    "            'frame_mapping': {}\n",
    "        }\n",
    "        \n",
    "        # Build frame mapping\n",
    "        for model_idx, original_frame_idx in enumerate(frame_indices):\n",
    "            original_timestamp = original_frame_idx / detected_fps\n",
    "            mapping_info['frame_mapping'][model_idx] = {\n",
    "                'original_frame_idx': int(original_frame_idx),\n",
    "                'original_timestamp': original_timestamp,\n",
    "                'timestamp_formatted': f\"{original_timestamp:.2f}s\"\n",
    "            }\n",
    "        \n",
    "        print(f\"Time scale factor: {mapping_info['time_scale_factor']:.2f}\")\n",
    "    \n",
    "    # Get video frames with updated frame count\n",
    "    video_file_path, frames, timestamps = get_video_frames(video_path, effective_num_frames, cache_dir)\n",
    "    \n",
    "    # Create messages with FPS information\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\n",
    "            \"type\": \"video\", \n",
    "            \"video\": video_path,\n",
    "            \"total_pixels\": total_pixels,\n",
    "            \"min_pixels\": min_pixels,\n",
    "            \"fps\": original_fps  # Pass original FPS to model\n",
    "        }]\n",
    "    }]\n",
    "    \n",
    "    # Process video for model input\n",
    "    image_inputs, video_inputs, video_kwargs = process_vision_info(messages, return_video_kwargs=True)\n",
    "    \n",
    "    # Update video_kwargs with our FPS information\n",
    "    if 'fps' not in video_kwargs or not video_kwargs['fps']:\n",
    "        video_kwargs['fps'] = [original_fps]\n",
    "    \n",
    "    inputs = processor(\n",
    "        text=[\"\"],  # Dummy text\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    inputs = inputs.to(model.device)\n",
    "    \n",
    "    # Extract embeddings using the visual encoder\n",
    "    with torch.no_grad():\n",
    "        # Method 1: Use the correct parameter names for Qwen2.5-VL\n",
    "        if hasattr(inputs, 'pixel_values_videos') and inputs.pixel_values_videos is not None:\n",
    "            # For video inputs\n",
    "            visual_outputs = model.visual(\n",
    "                inputs.pixel_values_videos,\n",
    "                grid_thw=inputs.get('video_grid_thw')\n",
    "            )\n",
    "        elif hasattr(inputs, 'pixel_values') and inputs.pixel_values is not None:\n",
    "            # For image inputs\n",
    "            visual_outputs = model.visual(inputs.pixel_values)\n",
    "        else:\n",
    "            raise ValueError(\"No valid pixel values found in inputs\")\n",
    "        \n",
    "        # Extract the embeddings\n",
    "        if hasattr(visual_outputs, 'last_hidden_state'):\n",
    "            vision_embeddings = visual_outputs.last_hidden_state\n",
    "        else:\n",
    "            # If direct access doesn't work, visual_outputs might be the embeddings directly\n",
    "            vision_embeddings = visual_outputs\n",
    "    \n",
    "    # Enhanced embeddings data with FPS and mapping info\n",
    "    embeddings_data = {\n",
    "        'vision_embeddings': vision_embeddings.cpu(),\n",
    "        'video_grid_thw': inputs.get('video_grid_thw'),\n",
    "        'fps': video_kwargs.get('fps', [original_fps]),\n",
    "        'original_fps': original_fps,\n",
    "        'detected_fps': detected_fps,\n",
    "        'num_frames': video_inputs[0].shape[0] if video_inputs else 0,\n",
    "        'effective_num_frames': effective_num_frames,\n",
    "        'total_frames': total_frames,\n",
    "        'duration': duration,\n",
    "        'include_mapping': include_mapping\n",
    "    }\n",
    "    \n",
    "    # Add timestamp conversion utilities to embeddings_data\n",
    "    if include_mapping and mapping_info:\n",
    "        embeddings_data.update({\n",
    "            'time_scale_factor': mapping_info['time_scale_factor'],\n",
    "            'timestamp_converter': {\n",
    "                'model_to_original_time': lambda t: t * mapping_info['time_scale_factor'],\n",
    "                'model_to_original_range': lambda start, end: (\n",
    "                    start * mapping_info['time_scale_factor'],\n",
    "                    end * mapping_info['time_scale_factor']\n",
    "                ),\n",
    "                'scale_factor': mapping_info['time_scale_factor']\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    # Cache the embeddings and mapping\n",
    "    torch.save(embeddings_data, embeddings_cache_file)\n",
    "    np.save(frames_cache_file, frames)\n",
    "    np.save(timestamps_cache_file, timestamps)\n",
    "    \n",
    "    if include_mapping and mapping_info:\n",
    "        import json\n",
    "        with open(mapping_cache_file, 'w') as f:\n",
    "            json.dump(mapping_info, f, indent=2)\n",
    "        print(f\"Cached mapping info to {mapping_cache_file}\")\n",
    "    \n",
    "    print(f\"Cached video embeddings to {embeddings_cache_file}\")\n",
    "    \n",
    "    if include_mapping:\n",
    "        return frames, timestamps, embeddings_data, mapping_info\n",
    "    else:\n",
    "        return frames, timestamps, embeddings_data\n",
    "\n",
    "\n",
    "def perform_video_grounding_with_cached_embeddings(\n",
    "    prompt: str, \n",
    "    model, \n",
    "    processor, \n",
    "    embeddings_data: Dict,\n",
    "    device: str = None\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Perform video grounding using pre-computed cached embeddings directly.\n",
    "    This bypasses video re-processing and uses the cached visual features.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Text query for video grounding\n",
    "        model: The Qwen2.5-VL model\n",
    "        processor: The processor/tokenizer\n",
    "        embeddings_data: Dict containing cached embeddings and metadata\n",
    "        device: Device to run inference on\n",
    "        \n",
    "    Returns:\n",
    "        Generated response text\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"=== Video Grounding with Cached Embeddings ===\")\n",
    "    print(f\"Query: {prompt}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    if device is None:\n",
    "        device = model.device\n",
    "    \n",
    "    # Extract cached embeddings\n",
    "    vision_embeddings = embeddings_data['vision_embeddings'].to(device)\n",
    "    video_grid_thw = embeddings_data.get('video_grid_thw')\n",
    "    if video_grid_thw is not None:\n",
    "        video_grid_thw = video_grid_thw.to(device)\n",
    "    \n",
    "    print(f\"âœ… Using cached embeddings:\")\n",
    "    print(f\"   Shape: {vision_embeddings.shape}\")\n",
    "    print(f\"   Frames: {embeddings_data.get('num_frames', 'N/A')}\")\n",
    "    print(f\"   Device: {vision_embeddings.device}\")\n",
    "    \n",
    "    # Tokenize the text prompt\n",
    "    messages = [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": prompt}]}]\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    \n",
    "    # Tokenize text only (no video processing)\n",
    "    text_inputs = processor.tokenizer(\n",
    "        text, \n",
    "        return_tensors=\"pt\", \n",
    "        padding=True,\n",
    "        truncation=True\n",
    "    ).to(device)\n",
    "    \n",
    "    print(f\"âœ… Text tokenized:\")\n",
    "    print(f\"   Input IDs shape: {text_inputs.input_ids.shape}\")\n",
    "    print(f\"   Text length: {len(text)}\")\n",
    "    \n",
    "    # Prepare inputs for the model\n",
    "    # We need to manually construct the input embeddings\n",
    "    with torch.no_grad():\n",
    "        # Get text embeddings\n",
    "        text_embeddings = model.get_input_embeddings()(text_inputs.input_ids)\n",
    "        \n",
    "        # Combine text and vision embeddings\n",
    "        # Vision embeddings typically go first, then text\n",
    "        if vision_embeddings.dim() == 2:\n",
    "            vision_embeddings = vision_embeddings.unsqueeze(0)  # Add batch dimension\n",
    "        \n",
    "        if text_embeddings.dim() == 3 and vision_embeddings.dim() == 3:\n",
    "            # Concatenate along sequence dimension\n",
    "            combined_embeddings = torch.cat([vision_embeddings, text_embeddings], dim=1)\n",
    "        else:\n",
    "            print(f\"âš ï¸  Dimension mismatch - Vision: {vision_embeddings.shape}, Text: {text_embeddings.shape}\")\n",
    "            combined_embeddings = text_embeddings\n",
    "        \n",
    "        print(f\"âœ… Combined embeddings shape: {combined_embeddings.shape}\")\n",
    "        \n",
    "        # Create attention mask for combined embeddings\n",
    "        vision_attention = torch.ones(vision_embeddings.shape[:2], device=device, dtype=torch.long)\n",
    "        text_attention = text_inputs.attention_mask\n",
    "        combined_attention_mask = torch.cat([vision_attention, text_attention], dim=1)\n",
    "        \n",
    "        # Prepare generation inputs\n",
    "        generation_inputs = {\n",
    "            'inputs_embeds': combined_embeddings,\n",
    "            'attention_mask': combined_attention_mask,\n",
    "        }\n",
    "        \n",
    "        # Add video-specific inputs if available\n",
    "        if video_grid_thw is not None:\n",
    "            generation_inputs['video_grid_thw'] = video_grid_thw\n",
    "        \n",
    "        print(\"ğŸ¤– Generating response with cached embeddings...\")\n",
    "        \n",
    "        # Generate response\n",
    "        generated_ids = model.generate(\n",
    "            **generation_inputs,\n",
    "            max_new_tokens=512,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=processor.tokenizer.eos_token_id,\n",
    "            eos_token_id=processor.tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "        # Decode only the generated part (skip the input embeddings part)\n",
    "        # Since we used inputs_embeds, we need to decode all generated tokens\n",
    "        response = processor.tokenizer.batch_decode(\n",
    "            generated_ids,\n",
    "            skip_special_tokens=True,\n",
    "            clean_up_tokenization_spaces=False\n",
    "        )[0]\n",
    "        \n",
    "        # Clean up the response (remove the input prompt if it appears)\n",
    "        if text in response:\n",
    "            response = response.replace(text, \"\").strip()\n",
    "        \n",
    "        print(f\"âœ… Response generated:\")\n",
    "        print(f\"ğŸ“ Model Response: {response}\")\n",
    "        \n",
    "        return response\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f330796d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video info: 189 frames, 12.00 FPS, 15.75s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused or unrecognized kwargs: return_tensors, fps.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached video embeddings to .cache/388b5a2524b105b3905d21b332243859_embeddings.pt\n",
      "FPS passed to model: 12.0\n"
     ]
    }
   ],
   "source": [
    "# Test FPS alone - no timestamp mapping\n",
    "video_path = \"/home/ubuntu/lilynogh/Qwen2.5-VL/cookbooks/downloads/downloaded_video.mp4\"\n",
    "\n",
    "frames, timestamps, embeddings_data = get_video_embeddings(\n",
    "    video_path=video_path,\n",
    "    model=model,\n",
    "    processor=processor,\n",
    "    original_fps=12.0,        # Pass your FPS\n",
    "    include_mapping=False,    # KEY: Disable mapping\n",
    "    num_frames=190           # Use original frame count\n",
    ")\n",
    "\n",
    "print(f\"FPS passed to model: {embeddings_data['original_fps']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c634ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded embeddings:\n",
      "  Shape: torch.Size([19646, 3584])\n",
      "  Frames: 188\n",
      "  FPS: [11.936507936507937]\n"
     ]
    }
   ],
   "source": [
    "embeddings_data = torch.load('.cache/388b5a2524b105b3905d21b332243859_embeddings.pt')\n",
    "\n",
    "print(f\"Loaded embeddings:\")\n",
    "print(f\"  Shape: {embeddings_data['vision_embeddings'].shape}\")\n",
    "print(f\"  Frames: {embeddings_data['num_frames']}\")\n",
    "print(f\"  FPS: {embeddings_data['fps']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1b33f457",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused or unrecognized kwargs: return_tensors, fps.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video processing verification:\n",
      "  Processed frames: 30\n",
      "188-frame response: The event you described, where \"other vehicle did lane changes into the lane in front me and I have to deaccelerate,\" occurs around the middle of the video, specifically between 3.0 seconds and 5.0 seconds. At this point, a black SUV can be seen making a lane change, which causes the vehicle from whose perspective the video is shot to slow down or decelerate.\n"
     ]
    }
   ],
   "source": [
    "# Test lane change detection with 188 frames\n",
    "video_path = \"/home/ubuntu/lilynogh/Qwen2.5-VL/cookbooks/downloads/downloaded_video.mp4\"\n",
    "prompt = \"Give the query: 'other vehicle did lane changes into the lane in front me and I have to deaccelerate', when does the described content occur in the video?\"\n",
    "\n",
    "# Use the standard perform_video_grounding function (the one that re-processes video)\n",
    "# but it will use the same parameters that created your 188-frame embeddings\n",
    "messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [\n",
    "        {\n",
    "            \"type\": \"video\", \n",
    "            \"video\": video_path,\n",
    "            \"total_pixels\": 20480 * 28 * 28,  # Your parameters\n",
    "            \"min_pixels\": 16 * 28 * 28\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"text\", \n",
    "            \"text\": prompt\n",
    "        }\n",
    "    ]\n",
    "}]\n",
    "\n",
    "# Process and generate\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "inputs = inputs.to(model.device)\n",
    "\n",
    "print(f\"Video processing verification:\")\n",
    "print(f\"  Processed frames: {video_inputs[0].shape[0] if video_inputs else 'None'}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9\n",
    "    )\n",
    "    \n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    response = processor.batch_decode(\n",
    "        generated_ids_trimmed, \n",
    "        skip_special_tokens=True, \n",
    "        clean_up_tokenization_spaces=False\n",
    "    )[0]\n",
    "\n",
    "print(f\"188-frame response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c9cd29e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Video Grounding with Cached Embeddings ===\n",
      "Query: Give the query: 'other vehicle did lane changes into the lane in front me and I have to deaccelerate', when does the described content occur in the video?\n",
      "------------------------------------------------------------\n",
      "âœ… Using cached embeddings:\n",
      "   Shape: torch.Size([19646, 3584])\n",
      "   Frames: 188\n",
      "   Device: cuda:0\n",
      "âœ… Text tokenized:\n",
      "   Input IDs shape: torch.Size([1, 53])\n",
      "   Text length: 262\n",
      "âœ… Combined embeddings shape: torch.Size([1, 19699, 3584])\n",
      "ğŸ¤– Generating response with cached embeddings...\n",
      "âœ… Response generated:\n",
      "ğŸ“ Model Response: The event where the described content occurs is when the other vehicle makes a left turn in front of you.\n",
      "Result: The event where the described content occurs is when the other vehicle makes a left turn in front of you.\n"
     ]
    }
   ],
   "source": [
    "embeddings_data = torch.load('.cache/388b5a2524b105b3905d21b332243859_embeddings.pt')\n",
    "prompt = \"Give the query: 'other vehicle did lane changes into the lane in front me and I have to deaccelerate', when does the described content occur in the video?\"\n",
    "\n",
    "# Use cached embeddings directly\n",
    "response = perform_video_grounding_with_cached_embeddings(\n",
    "    prompt=prompt,\n",
    "    model=model,\n",
    "    processor=processor,\n",
    "    embeddings_data=embeddings_data\n",
    ")\n",
    "print(f\"Result: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49115403",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"/home/ubuntu/lilynogh/Qwen2.5-VL/cookbooks/downloads/downloaded_video.mp4\"\n",
    "your_prompt = \"Give the query: 'other vehicle did lane changes into the lane in front me and I have to deaccelerate', when does the described content occur in the video?\"\n",
    "\n",
    "embeddings_data = torch.load('.cache/388b5a2524b105b3905d21b332243859_embeddings.pt')\n",
    "response = perform_video_grounding(video_path, your_prompt, model, processor, embeddings_data)\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d3e08e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Video Grounding Validation ===\n",
      "Query: Give the query: 'other vehicle did lane changes into the lane in front me and I have to deaccelerate', when does the described content occur in the video?\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused or unrecognized kwargs: return_tensors, fps.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Response: The event you're describing, where another vehicle makes a lane change into your lane and you need to decelerate, occurs around 0.0 - 5.0 seconds in the video. Specifically, a black SUV can be seen making a lane change into the lane in front of the camera, causing the camera's perspective to move forward as it follows the traffic flow.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The event you're describing, where another vehicle makes a lane change into your lane and you need to decelerate, occurs around 0.0 - 5.0 seconds in the video. Specifically, a black SUV can be seen making a lane change into the lane in front of the camera, causing the camera's perspective to move forward as it follows the traffic flow."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "video_path = \"/home/ubuntu/lilynogh/Qwen2.5-VL/cookbooks/downloads/downloaded_video.mp4\"\n",
    "your_prompt = \"Give the query: 'other vehicle did lane changes into the lane in front me and I have to deaccelerate', when does the described content occur in the video?\"\n",
    "\n",
    "embeddings_data = torch.load('.cache/d9e492fa1ea59dc77cb44cf272647efa_embeddings.pt')\n",
    "response = perform_video_grounding(video_path, your_prompt, model, processor, embeddings_data)\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b1b9da54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused or unrecognized kwargs: return_tensors, fps.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached video embeddings to .cache/d9e492fa1ea59dc77cb44cf272647efa_embeddings.pt\n",
      "Video embeddings shape: torch.Size([10800, 3584])\n",
      "Number of frames: 30\n",
      "FPS: [1.9047619047619047]\n"
     ]
    }
   ],
   "source": [
    "video_path = \"/home/ubuntu/lilynogh/Qwen2.5-VL/cookbooks/downloads/downloaded_video.mp4\"\n",
    "\n",
    "frames, timestamps, embeddings_data = get_video_embeddings(\n",
    "    video_path=video_path,\n",
    "    model=model,\n",
    "    processor=processor,\n",
    "    num_frames=160,\n",
    "    cache_dir='.cache'\n",
    ")\n",
    "\n",
    "print(f\"Video embeddings shape: {embeddings_data['vision_embeddings'].shape}\")\n",
    "print(f\"Number of frames: {embeddings_data['num_frames']}\")\n",
    "print(f\"FPS: {embeddings_data['fps']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5974ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused or unrecognized kwargs: return_tensors, fps.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video input: torch.Size([30, 3, 560, 1008])\n",
      "num of video tokens: 10800\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The described content, \"other vehicle did lane changes into the lane in front me and I have to deaccelerate,\" occurs around 0:04-0:12 in the video. At this point, a black car is seen making a lane change directly in front of the camera's perspective, causing the vehicle from which the footage is being recorded to slow down or deaccelerate."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# long video understanding\n",
    "# prompt = \"Could you go into detail about the content of this driving video?\"\n",
    "\n",
    "# video summarization\n",
    "# prompt = \"Could you use a table to summarize the interesting vehicle driving behaviors in this driving video?\"\n",
    "\n",
    "# video grounding\n",
    "prompt = \"Give the query: 'other vehicle did lane changes into the lane in front me and I have to deaccelerate', when does the described content occur in the video?\"\n",
    "\n",
    "\n",
    "response = inference(\"/home/ubuntu/lilynogh/Qwen2.5-VL/cookbooks/downloads/downloaded_video.mp4\", prompt)\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ffbbc2",
   "metadata": {},
   "source": [
    "Inference function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ef59a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(video_path, prompt, max_new_tokens=2048, total_pixels=20480 * 28 * 28, min_pixels=16 * 28 * 28):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"text\", \"text\": prompt},\n",
    "                {\"video\": video_path, \"total_pixels\": total_pixels, \"min_pixels\": min_pixels},\n",
    "            ]\n",
    "        },\n",
    "    ]\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    image_inputs, video_inputs, video_kwargs = process_vision_info([messages], return_video_kwargs=True)\n",
    "    fps_inputs = video_kwargs['fps']\n",
    "    print(\"video input:\", video_inputs[0].shape)\n",
    "    num_frames, _, resized_height, resized_width = video_inputs[0].shape\n",
    "    print(\"num of video tokens:\", int(num_frames / 2 * resized_height / 28 * resized_width / 28))\n",
    "    inputs = processor(text=[text], images=image_inputs, videos=video_inputs, fps=fps_inputs, padding=True, return_tensors=\"pt\")\n",
    "    inputs = inputs.to('cuda')\n",
    "\n",
    "    output_ids = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "    generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]\n",
    "    output_text = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    return output_text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5057b6a8",
   "metadata": {},
   "source": [
    "Inference function with API using OpenAI SDK.\n",
    "\n",
    "**Important Notice:**\n",
    "- Please be aware that the current API supports video processing up to a maximum length of 10 minutes.\n",
    "- Currently, the model inference interface does not support configuring the resolution of video frames. Therefore, it is recommended to resize videos with higher resolutions and longer durations to a smaller resolution to ensure that the input sequence is not excessively long. We advise keeping the number of video tokens under 24k to achieve better video grounding results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c0a3ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from openai import OpenAI\n",
    "# from IPython.display import Markdown, display\n",
    "\n",
    "\n",
    "# def inference_with_api(\n",
    "#     video_path,\n",
    "#     prompt,\n",
    "#     sys_prompt = \"You are a helpful assistant.\",\n",
    "#     model_id = \"qwen-vl-max-latest\",\n",
    "# ):\n",
    "#     client = OpenAI(\n",
    "#         api_key = os.getenv('DASHSCOPE_API_KEY'),\n",
    "#         base_url = \"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    "#     )    \n",
    "#     messages = [\n",
    "#         {\n",
    "#             \"role\": \"system\",\n",
    "#             \"content\": [{\"type\":\"text\",\"text\": sys_prompt}]\n",
    "#         },\n",
    "#         {\n",
    "#             \"role\": \"user\",\n",
    "#             \"content\": [\n",
    "#                 {\"type\": \"video_url\", \"video_url\": {\"url\": video_path}},\n",
    "#                 {\"type\": \"text\", \"text\": prompt},\n",
    "#             ]\n",
    "#         }\n",
    "#     ]\n",
    "#     completion = client.chat.completions.create(\n",
    "#         model = model_id,\n",
    "#         messages = messages,\n",
    "#     )\n",
    "#     print(completion)\n",
    "#     return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58729648",
   "metadata": {},
   "source": [
    "#### 1. Reading Text in Videos\n",
    "\n",
    "In this section, we demonstrate how the model can be used to recognize and summarize text within a video. Specifically, we'll use a video containing various products and ask the model to summarize their characteristics in a structured format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e0ff708",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gdown\n",
      "  Downloading gdown-5.2.0-py3-none-any.whl (18 kB)\n",
      "Collecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.13.4-py3-none-any.whl (187 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m187.3/187.3 KB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /lambda/nfs/lilynogh/qwen_venv/lib/python3.10/site-packages (from gdown) (4.67.1)\n",
      "Requirement already satisfied: filelock in /lambda/nfs/lilynogh/qwen_venv/lib/python3.10/site-packages (from gdown) (3.18.0)\n",
      "Requirement already satisfied: requests[socks] in /lambda/nfs/lilynogh/qwen_venv/lib/python3.10/site-packages (from gdown) (2.32.4)\n",
      "Collecting soupsieve>1.2\n",
      "  Downloading soupsieve-2.7-py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /lambda/nfs/lilynogh/qwen_venv/lib/python3.10/site-packages (from beautifulsoup4->gdown) (4.14.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /lambda/nfs/lilynogh/qwen_venv/lib/python3.10/site-packages (from requests[socks]->gdown) (3.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /lambda/nfs/lilynogh/qwen_venv/lib/python3.10/site-packages (from requests[socks]->gdown) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /lambda/nfs/lilynogh/qwen_venv/lib/python3.10/site-packages (from requests[socks]->gdown) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /lambda/nfs/lilynogh/qwen_venv/lib/python3.10/site-packages (from requests[socks]->gdown) (2025.6.15)\n",
      "Collecting PySocks!=1.5.7,>=1.5.6\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: soupsieve, PySocks, beautifulsoup4, gdown\n",
      "Successfully installed PySocks-1.7.1 beautifulsoup4-4.13.4 gdown-5.2.0 soupsieve-2.7\n"
     ]
    }
   ],
   "source": [
    "# !pip uninstall transformers\n",
    "# !pip install transformers==4.52\n",
    "!pip install gdown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e185122",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1xpBUYOO9vjr0eQiDAYlPhG6UC6CLyukK\n",
      "To: /lambda/nfs/lilynogh/Qwen2.5-VL/cookbooks/downloads/downloaded_video.mp4\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10.7M/10.7M [00:00<00:00, 33.2MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded to: ./downloads/downloaded_video.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import gdown\n",
    "# Your Google Drive URL\n",
    "url = \"https://drive.google.com/file/d/1xpBUYOO9vjr0eQiDAYlPhG6UC6CLyukK/view?usp=sharing\"\n",
    "\n",
    "# Extract the file ID from the URL\n",
    "file_id = \"1xpBUYOO9vjr0eQiDAYlPhG6UC6CLyukK\"\n",
    "\n",
    "# Create local directory if it doesn't exist\n",
    "local_dir = \"./downloads\"\n",
    "os.makedirs(local_dir, exist_ok=True)\n",
    "\n",
    "# Download the file\n",
    "output_path = os.path.join(local_dir, \"downloaded_video.mp4\")\n",
    "gdown.download(f\"https://drive.google.com/uc?id={file_id}\", output_path, quiet=False)\n",
    "\n",
    "print(f\"File downloaded to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ddf6a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded to directory: .cache\n"
     ]
    }
   ],
   "source": [
    "# url = \"https://drive.google.com/uc?export=download&id=1xpBUYOO9vjr0eQiDAYlPhG6UC6CLyukK\"\n",
    "# download_dir = quick_download(url)\n",
    "# print(f\"Downloaded to directory: {download_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6648a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused or unrecognized kwargs: return_tensors, fps.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video input: torch.Size([30, 3, 560, 1008])\n",
      "num of video tokens: 10800\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The described content, \"other vehicle did lane changes into the lane in front me and I have to deaccelerate,\" occurs around 0:04-0:12 in the video. At this point, a black car is seen making a lane change directly in front of the camera's perspective, causing the vehicle from which the footage is being recorded to slow down or deaccelerate."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# long video understanding\n",
    "# prompt = \"Could you go into detail about the content of this driving video?\"\n",
    "\n",
    "# video summarization\n",
    "# prompt = \"Could you use a table to summarize the interesting vehicle driving behaviors in this driving video?\"\n",
    "\n",
    "# video grounding\n",
    "prompt = \"Give the query: 'other vehicle did lane changes into the lane in front me and I have to deaccelerate', when does the described content occur in the video?\"\n",
    "\n",
    "\n",
    "response = inference(\"/home/ubuntu/lilynogh/Qwen2.5-VL/cookbooks/downloads/downloaded_video.mp4\", prompt)\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a94240e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path, frames, timestamps = get_video_frames(\"/home/ubuntu/lilynogh/Qwen2.5-VL/cookbooks/downloads/downloaded_video.mp4\", num_frames=192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d1e233a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "qwen-vl-utils using decord to read video.\n",
      "Unused or unrecognized kwargs: return_tensors, fps.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video input: torch.Size([58, 3, 532, 980])\n",
      "num of video tokens: 19285\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "ä»¥ä¸‹æ˜¯æ ¹æ®è§†é¢‘å†…å®¹æ€»ç»“çš„å•†å“ç‰¹ç‚¹è¡¨æ ¼ï¼š\n",
       "\n",
       "| ç‰¹ç‚¹ | æè¿° |\n",
       "|------|------|\n",
       "| é€‚ç”¨èŒƒå›´å¹¿ | å¯ç”¨äºé¾™çœ¼ã€åˆ‡ç‰‡è¥¿ç“œã€åœ£å¥³æœã€æ¨±æ¡ƒç­‰å¤šç§æ°´æœåŒ…è£…ã€‚ |\n",
       "| ææ‰£è®¾è®¡ | äººæ€§åŒ–è®¾è®¡ï¼Œæ˜“æ‰£ä¸ç¹çã€‚ |\n",
       "| ææ‰£ç´§é” | ä¸Šä¸‹ç›–ç´§é”ï¼Œæ‘‡æ™ƒä¸è„±è½ã€‚ |\n",
       "| ä¸“ä¸šé“è†œ | é‡‡ç”¨PETææ–™åˆ¶ä½œï¼Œåšå·¥ç²¾ç»†ã€‚ |\n",
       "| é˜²å‹æŠ—æ‘” | è€å‹è€ç£¨ï¼Œè€ä½æ¸©ï¼Œå¯å†·è—ã€‚ |\n",
       "| ç¾è§‚å®ç”¨ | çº¹ç†æ¸…æ™°è´¨æ„Ÿä½³ï¼Œå½¢çŠ¶å¥½ï¼Œå…‰æ³½åº¦å¥½ã€‚ |\n",
       "| é«˜é€åŠ åš | ç›’å†…äº§å“ä¸€ç›®äº†ç„¶ï¼Œæ— è‰²æ— å‘³ã€‚ |\n",
       "| å…¨é¢å±•ç¤º | å…¨é¢å±•ç¤ºäº§å“ç»†èŠ‚ã€‚ |\n",
       "\n",
       "å¸Œæœ›è¿™ä¸ªè¡¨æ ¼èƒ½å¸®åŠ©ä½ æ›´å¥½åœ°ç†è§£è§†é¢‘ä¸­å•†å“çš„ç‰¹ç‚¹ï¼"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "video_url = \"https://duguang-labelling.oss-cn-shanghai.aliyuncs.com/qiansun/video_ocr/videos/50221078283.mp4\"\n",
    "prompt = \"è¯·ç”¨è¡¨æ ¼æ€»ç»“ä¸€ä¸‹è§†é¢‘ä¸­çš„å•†å“ç‰¹ç‚¹\"\n",
    "\n",
    "## Use a local HuggingFace model to inference.\n",
    "video_path, frames, timestamps = get_video_frames(video_url, num_frames=64)\n",
    "# image_grid = create_image_grid(frames, num_columns=8)\n",
    "# display(image_grid.resize((640, 640)))\n",
    "\n",
    "response = inference(video_path, prompt)\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970b12d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use API for inference. Apply API key here: https://bailian.console.alibabacloud.com/?apiKey=1\n",
    "# os.environ['DASHSCOPE_API_KEY'] = 'your_api_key_here' \n",
    "\n",
    "video_url = \"http://ofasys-multimodal-wlcb-3.oss-cn-wulanchabu.aliyuncs.com/sibo.ssb/cookbook/video_ocr.mp4\"\n",
    "prompt = \"Watch the video and list the paper titles in a table, add one extra column for translating the paper titles to Chinese.\"\n",
    "\n",
    "response = inference_with_api(video_url, prompt)\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f368545",
   "metadata": {},
   "source": [
    "#### 2. Long Video Understanding\n",
    "\n",
    "Next, we explore the model's capability to comprehend extremely long videos, such as those lasting up to one hour. This demonstrates how the model can effectively process and analyze extended video content, extracting meaningful insights over longer durations.\n",
    "\n",
    "To reduce the number of visual tokens generated from a long video, you can specify the `resized_height` and `resized_width` parameters. These settings allow the video frames to be resized to a smaller dimension, effectively decreasing the computational load while maintaining the essential visual information needed for analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030041db",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_url = \"https://ofasys-multimodal-wlcb-3.oss-cn-wulanchabu.aliyuncs.com/sibo.ssb/evaluations/data/LVBench/videos/GcRKREorGSc.mp4\"\n",
    "prompt = \"Could you go into detail about the content of this long video?\"\n",
    "\n",
    "video_path, frames, timestamps = get_video_frames(video_url, num_frames=64)\n",
    "# image_grid = create_image_grid(frames, num_columns=8)\n",
    "# display(image_grid.resize((640, 640)))\n",
    "\n",
    "response = inference(video_path, prompt)\n",
    "display(Markdown(response))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610b0b05",
   "metadata": {},
   "source": [
    "#### 3. Video Grounding\n",
    "\n",
    "This part focuses on answering specific questions about a video segment. We specify a textual query and ask the model what is the period that the described content occur in the video, showcasing the model's ability to understand timestamps and search the detailed queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebe81ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_url = \"https://ofasys-multimodal-wlcb-3.oss-cn-wulanchabu.aliyuncs.com/sibo.ssb/datasets/cookbook/ead2e3f0e7f836c9ec51236befdaf2d843ac13a6.mp4\"\n",
    "prompt = \"Give the query: 'seasoning the steak', when does the described content occur in the video?\"\n",
    "\n",
    "video_path, frames, timestamps = get_video_frames(video_url, num_frames=64)\n",
    "image_grid = create_image_grid(frames, num_columns=8)\n",
    "display(image_grid.resize((640, 640)))\n",
    "\n",
    "# inference\n",
    "response = inference(video_path, prompt)\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9228a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use API for inference. Apply API key here: https://bailian.console.alibabacloud.com/?apiKey=1\n",
    "# os.environ['DASHSCOPE_API_KEY'] = 'your_api_key_here' \n",
    "\n",
    "video_url = \"http://ofasys-multimodal-wlcb-3.oss-cn-wulanchabu.aliyuncs.com/sibo.ssb/cookbook/video_structured_caption_480p.mov\"\n",
    "prompt = \"Give the query: 'The seasoned meat is placed on a grill', when does the described content occur in the video? Use â€˜mm:ss.ffâ€™ as time format.\"\n",
    "\n",
    "response = inference_with_api(video_url, prompt)\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec0cab5",
   "metadata": {},
   "source": [
    "#### 4. Structured Video Captioning\n",
    "\n",
    "Finally, we present a scenario where the model identifies significant events within the video, providing start and end timestamps for each event along with descriptive sentences. The output is formatted in JSON for easy parsing and further processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd629f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_url = \"https://ofasys-multimodal-wlcb-3.oss-cn-wulanchabu.aliyuncs.com/sibo.ssb/datasets/cookbook/ead2e3f0e7f836c9ec51236befdaf2d843ac13a6.mp4\"\n",
    "prompt = \"Localize a series of activity events in the video, output the start and end timestamp for each event, and describe each event with sentences. Provide the result in json format with 'mm:ss.ff' format for time depiction.\"\n",
    "\n",
    "video_path, frames, timestamps = get_video_frames(video_url, num_frames=64)\n",
    "# image_grid = create_image_grid(frames, num_columns=8)\n",
    "# display(image_grid.resize((640, 640)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e24e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference\n",
    "response = inference(video_path, prompt)\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f485089",
   "metadata": {},
   "source": [
    "- By post-processing the json results, we can intuitively present video clips and descriptions in an interleaved manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9beb0368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def parse_json(response):\n",
    "    html = markdown.markdown(response, extensions=['fenced_code'])\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    json_text = soup.find('code').text\n",
    "\n",
    "    data = json.loads(json_text)\n",
    "    return data\n",
    "\n",
    "\n",
    "def time_to_seconds(time_str):\n",
    "    time_obj = datetime.strptime(time_str, '%M:%S.%f')\n",
    "    total_seconds = time_obj.minute * 60 + time_obj.second + time_obj.microsecond / 1_000_000\n",
    "    return total_seconds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14354bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = parse_json(response)\n",
    "\n",
    "for item in data:\n",
    "    start_time = item[\"start_time\"]\n",
    "    end_time = item[\"end_time\"]\n",
    "    description = item[\"description\"]\n",
    "\n",
    "    display(Markdown(f\"**{start_time} - {end_time}:**\\t\\t\" + description))\n",
    "\n",
    "    start_time = time_to_seconds(start_time)\n",
    "    end_time = time_to_seconds(end_time)\n",
    "    current_frames = []\n",
    "    for frame, timestamp in zip(frames, timestamps):\n",
    "        if timestamp[0] > start_time and timestamp[1] < end_time:\n",
    "            current_frames.append(frame)\n",
    "    \n",
    "    current_frames = np.array(current_frames)\n",
    "    current_image_grid = create_image_grid(current_frames, num_columns=8)\n",
    "\n",
    "    display(current_image_grid.resize((480, (int(len(current_frames) / 8) + 1) * 60)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cd4e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use API for inference. Apply API key here: https://bailian.console.alibabacloud.com/?apiKey=1\n",
    "# os.environ['DASHSCOPE_API_KEY'] = 'your_api_key_here' \n",
    "\n",
    "video_url = \"http://ofasys-multimodal-wlcb-3.oss-cn-wulanchabu.aliyuncs.com/sibo.ssb/cookbook/video_structured_caption_480p.mov\"\n",
    "prompt = \"Localize a series of activity events in the video, output the start and end timestamp for each event, and describe each event with sentences. Provide the result in json format with â€˜mm:ss.ffâ€™ format for time depiction.\"\n",
    "\n",
    "response = inference_with_api(video_url, prompt)\n",
    "display(Markdown(response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwen_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
