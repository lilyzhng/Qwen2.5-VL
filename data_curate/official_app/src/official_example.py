# -*- coding: utf-8 -*-
# Ignore this code
"""example.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//huggingface.co/nvidia/Cosmos-Embed1-448p/blob/main/examples/example.ipynb

## Setup

We need `transformers`, `torchvision` and `einops` as basic dependencies for the model.
For this example, we also use `wget` for fetching data remotely, `decord` for decoding video frames, and `mediapy` for saving videos.
"""

!pip install transformers torchvision einops decord mediapy

import decord
import numpy as np
import torch
from transformers import AutoConfig, AutoModel, AutoProcessor
from IPython.display import Video
import subprocess
import io

"""## Instantiate model

We use `AutoModel` and `AutoProcessor` to download the weights and inference code for Cosmos-Embed1. The model has been trained with bfloat16, so we should cast if the GPU supports it. The preprocessor tokenizes text and resizes/rescales batched video frames. We also override the default resolution to a non-square example.
"""

path = "nvidia/Cosmos-Embed1-448p"
resolution_override = (588, 672)  # ideally divisible by patch size 14

config = AutoConfig.from_pretrained(path, trust_remote_code=True)
config.resolution = resolution_override

model = AutoModel.from_pretrained(path, trust_remote_code=True, config=config).to("cuda", dtype=torch.bfloat16)
model.eval()
preprocess = AutoProcessor.from_pretrained(path, resolution=resolution_override, trust_remote_code=True)

"""## Fetch data"""

video_url = "https://upload.wikimedia.org/wikipedia/commons/3/3d/Branko_Paukovic%2C_javelin_throw.webm"
subprocess.check_call(["wget", "-O", "/tmp/output.mp4", video_url])
video_bytes = open("/tmp/output.mp4", "rb").read()
assert video_bytes
Video(video_url)

"""We sample 8 frames from the single video and create a tensor of shape `batch_size x num_frames x channel_dim x height x width`. The model has been trained on 8 frames sampled at 1-2FPS. For this example, we linearly sample frames from the entire ~2s clip."""

with io.BytesIO(video_bytes) as fp:
    reader = decord.VideoReader(fp)
    frame_ids = np.linspace(0, len(reader)-1, 8, dtype=int).tolist()
    frames = reader.get_batch(frame_ids).asnumpy()
batch = np.transpose(np.expand_dims(frames, 0), (0, 1, 4, 2, 3))  # BTCHW

"""## Inference

We run inference on the video batch by preprocessing it, moving it to the GPU and calling the `get_video_embeddings` method.

We run inference on text captions by preprocessing them into tokens and attention masks, moving to the GPU and calling the `get_text_embeddings` method.

We can then calculate the similarity between the text and video embeddings using a dot-product, and rank the captions by highest similarity to the video. The model correctly ranks the most likely caption as being `a man wearing red spandex throwing a javelin`.
"""

processed = preprocess(
    text=None,
    videos=batch,
    return_tensors="pt"
)
video_tensor = processed["videos"].to("cuda", dtype=torch.bfloat16)
with torch.no_grad():
    video_out = model.get_video_embeddings(video_tensor)

captions = [
    "a person riding a motorcycle in the night",
    "a car overtaking a white truck",
    "a video of a knight fighting with a sword",
    "a man wearing red spandex throwing a javelin",
    "a young man javelin throwing during the evening", # distractor
    "a man throwing a javelin with both hands", # distractor
]
text_inputs = preprocess(
    text=captions,
    return_tensors="pt"
).to("cuda")
with torch.no_grad():
    text_out = model.get_text_embeddings(
        input_ids=text_inputs["input_ids"],
        attention_mask=text_inputs["attention_mask"]
    )

probs = (torch.softmax(model.logit_scale.exp() * video_out.visual_proj @ text_out.text_proj.T, dim=-1))[0]
print(captions[probs.argmax()])

"""## Intermediate feature maps

We can also display the intermediate per-frame dense feature maps, displaying temporal stability and separability.
"""

import torch.nn.functional as F

def get_pca_map(
    feature_map: torch.Tensor,
    img_size,
    interpolation="bicubic",
    return_pca_stats=False,
    pca_stats=None,
    skip_components: int = 0,
):
    if feature_map.shape[0] != 1:
        feature_map = feature_map[None]
    if pca_stats is None:
        reduct_mat, color_min, color_max = get_robust_pca(
            feature_map.reshape(-1, feature_map.shape[-1]), skip=skip_components,
        )
    else:
        reduct_mat, color_min, color_max = pca_stats
    pca_color = feature_map @ reduct_mat
    pca_color = (pca_color - color_min) / (color_max - color_min)
    pca_color = pca_color.clamp(0, 1)
    pca_color = F.interpolate(
        pca_color.permute(0, 3, 1, 2),
        size=img_size,
        mode=interpolation,
    ).permute(0, 2, 3, 1)
    pca_color = pca_color.cpu().numpy().squeeze(0)
    if return_pca_stats:
        return pca_color, (reduct_mat, color_min, color_max)
    return pca_color


def get_robust_pca(features: torch.Tensor, m: float = 2, remove_first_component=False, skip: int = 0):
    assert len(features.shape) == 2, "features should be (N, C)"
    reduction_mat = torch.pca_lowrank(features, q=3 + skip, niter=20)[2]
    reduction_mat = reduction_mat[:, skip:]
    colors = features @ reduction_mat
    if remove_first_component:
        colors_min = colors.min(dim=0).values
        colors_max = colors.max(dim=0).values
        tmp_colors = (colors - colors_min) / (colors_max - colors_min)
        fg_mask = tmp_colors[..., 0] < 0.2
        reduction_mat = torch.pca_lowrank(features[fg_mask], q=3, niter=20)[2]
        colors = features @ reduction_mat
    else:
        fg_mask = torch.ones_like(colors[:, 0]).bool()
    d = torch.abs(colors[fg_mask] - torch.median(colors[fg_mask], dim=0).values)
    mdev = torch.median(d, dim=0).values
    s = d / mdev
    try:
        rins = colors[fg_mask][s[:, 0] < m, 0]
        gins = colors[fg_mask][s[:, 1] < m, 1]
        bins = colors[fg_mask][s[:, 2] < m, 2]
        rgb_min = torch.tensor([rins.min(), gins.min(), bins.min()])
        rgb_max = torch.tensor([rins.max(), gins.max(), bins.max()])
    except:
        rins = colors
        gins = colors
        bins = colors
        rgb_min = torch.tensor([rins.min(), gins.min(), bins.min()])
        rgb_max = torch.tensor([rins.max(), gins.max(), bins.max()])

    return reduction_mat, rgb_min.to(reduction_mat), rgb_max.to(reduction_mat)

video = "/tmp/output.mp4"
input_frames = np.stack([x.asnumpy() for x in decord.VideoReader(video)])
input_frames = torch.from_numpy(np.transpose(input_frames, (0, 3, 1, 2)))
print(input_frames.shape)

from einops import rearrange

num_frames = 8
num_batches = len(input_frames) // num_frames
batches = preprocess(videos=rearrange(input_frames, "(b t) c h w -> b t c h w", b=num_batches, t=num_frames)).to("cuda", dtype=torch.bfloat16)
with torch.no_grad():
    dense_features = torch.stack([
        model.get_video_embeddings(videos=inp).visual_embs[0]
        for inp in batches["videos"]
    ])
dense_features = rearrange(dense_features, "b t h w c -> (b t) h w c").to("cpu", dtype=torch.float32)
print(dense_features.shape)

"""Find PCA components of dense features for visualization purposes"""

num_keyframes = 30
kf_stride = max(dense_features.shape[0] // num_keyframes, 1)
sampled_features = dense_features[::kf_stride]
pca_stats = get_robust_pca(sampled_features.flatten(0, 2))
original_frames = input_frames.permute((0, 2, 3, 1)).cpu()

output_frames = []
for raw_frame, features in zip(original_frames, dense_features, strict=True):
    pca_features = get_pca_map(features, raw_frame.shape[0:2], pca_stats=pca_stats, interpolation="bilinear")
    pca_features = np.floor(pca_features * 255.0).astype(np.uint8)
    pca_features = np.concatenate((raw_frame, pca_features), 1)
    output_frames.append(pca_features)
output_frames = np.stack(output_frames)

import mediapy

viz_file = "/tmp/output-visualization.mp4"
mediapy.write_video(viz_file, output_frames, fps=30, codec="libx264")
Video(viz_file, embed=True)

